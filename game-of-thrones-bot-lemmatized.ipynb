{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ansfile.txt', 'quesfile.txt', 'finalgot.txt']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import SimpleRNN\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "INPUT_LENGTH = 20\n",
    "OUTPUT_LENGTH = 20\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "fn1 = open('../input/quesfile.txt', 'r')\n",
    "short_questions = fn1.readlines()\n",
    "short_questions = [x.lower() for x in short_questions]\n",
    "fn1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()\n",
    "lemmatized = [[lmtzr.lemmatize(word) for word in word_tokenize(s)]for s in short_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_questions = []\n",
    "for sentence in lemmatized:\n",
    "    joined_together = ' '.join(sentence)\n",
    "    short_questions.append(joined_together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn2 = open('../input/ansfile.txt', 'r')\n",
    "short_answers = fn2.readlines()\n",
    "short_answers = [x.lower() for x in short_answers]\n",
    "fn2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizeds = [[lmtzr.lemmatize(word) for word in word_tokenize(s)]for s in short_answers]\n",
    "\n",
    "short_answers = []\n",
    "for sentence in lemmatizeds:\n",
    "    joined_together = ' '.join(sentence)\n",
    "    short_answers.append(joined_together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing number of samples\n",
    "num_samples = 30000  # Number of samples to train on.\n",
    "short_questions = short_questions[:num_samples]\n",
    "short_answers = short_answers[:num_samples]\n",
    "#tokenizing the qns and answers\n",
    "short_questions_tok = [nltk.word_tokenize(sent) for sent in short_questions]\n",
    "short_answers_tok = [nltk.word_tokenize(sent) for sent in short_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size 9630\n",
      "validation size 2407\n"
     ]
    }
   ],
   "source": [
    "#train-validation split\n",
    "data_size = len(short_questions_tok)\n",
    "\n",
    "# We will use the first 0-80th %-tile (80%) of data for the training\n",
    "training_input  = short_questions_tok[:round(data_size*(80/100))]\n",
    "training_input  = [tr_input[::-1] for tr_input in training_input] #reverseing input seq for better performance\n",
    "training_output = short_answers_tok[:round(data_size*(80/100))]\n",
    "\n",
    "# We will use the remaining for validation\n",
    "validation_input = short_questions_tok[round(data_size*(80/100)):]\n",
    "validation_input  = [val_input[::-1] for val_input in validation_input] #reverseing input seq for better performance\n",
    "validation_output = short_answers_tok[round(data_size*(80/100)):]\n",
    "\n",
    "print('training size', len(training_input))\n",
    "print('validation size', len(validation_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for the frequency of the vocabulary\n",
    "# Create \n",
    "vocab = {}\n",
    "for question in short_questions_tok:\n",
    "    for word in question:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "\n",
    "for answer in short_answers_tok:\n",
    "    for word in answer:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare words from the vocabulary.\n",
    "# We will aim to replace fewer than 5% of words with <UNK>\n",
    "# You will see this ratio soon.\n",
    "threshold = 2\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of total vocab: 8401\n",
      "Size of vocab we will use: 5190\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of total vocab:\", len(vocab))\n",
    "print(\"Size of vocab we will use:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of vocab used: 5192\n"
     ]
    }
   ],
   "source": [
    "#we will create dictionaries to provide a unique integer for each word.\n",
    "WORD_CODE_START = 1\n",
    "WORD_CODE_PADDING = 0\n",
    "\n",
    "word_num  = 2 #number 1 is left for WORD_CODE_START for model decoder later\n",
    "encoding = {}\n",
    "decoding = {1: 'START'}\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold: #get vocabularies that appear above threshold count\n",
    "        encoding[word] = word_num \n",
    "        decoding[word_num ] = word\n",
    "        word_num += 1\n",
    "\n",
    "print(\"No. of vocab used:\", word_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5193"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#include unknown token for words not in dictionary\n",
    "decoding[len(encoding)+2] = '<UNK>'\n",
    "encoding['<UNK>'] = len(encoding)+2\n",
    "\n",
    "dict_size = word_num+1\n",
    "dict_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(encoding, data, vector_size=20):\n",
    "    transformed_data = np.zeros(shape=(len(data), vector_size))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(min(len(data[i]), vector_size)):\n",
    "            try:\n",
    "                transformed_data[i][j] = encoding[data[i][j]]\n",
    "            except:\n",
    "                transformed_data[i][j] = encoding['<UNK>']\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_training_input (9630, 20)\n",
      "encoded_training_output (9630, 20)\n"
     ]
    }
   ],
   "source": [
    "#encoding training set\n",
    "encoded_training_input = transform(\n",
    "    encoding, training_input, vector_size=INPUT_LENGTH)\n",
    "encoded_training_output = transform(\n",
    "    encoding, training_output, vector_size=OUTPUT_LENGTH)\n",
    "\n",
    "print('encoded_training_input', encoded_training_input.shape)\n",
    "print('encoded_training_output', encoded_training_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_validation_input (2406, 20)\n",
      "encoded_validation_output (2406, 20)\n"
     ]
    }
   ],
   "source": [
    "validation_input.pop()\n",
    "#encoding validation set\n",
    "encoded_validation_input = transform(\n",
    "    encoding, validation_input, vector_size=INPUT_LENGTH)\n",
    "encoded_validation_output = transform(\n",
    "    encoding, validation_output, vector_size=OUTPUT_LENGTH)\n",
    "\n",
    "print('encoded_validation_input', encoded_validation_input.shape)\n",
    "print('encoded_validation_output', encoded_validation_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "encoder_input = Input(shape=(INPUT_LENGTH,))\n",
    "decoder_input = Input(shape=(OUTPUT_LENGTH,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "encoder Tensor(\"lstm_1/transpose_2:0\", shape=(?, 20, 512), dtype=float32)\n",
      "encoder_last Tensor(\"strided_slice:0\", shape=(?, 512), dtype=float32)\n",
      "decoder Tensor(\"lstm_2/transpose_2:0\", shape=(?, 20, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "encoder = Embedding(dict_size, 128, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)\n",
    "encoder = LSTM(512, return_sequences=True, unroll=True)(encoder)\n",
    "encoder_last = encoder[:,-1,:]\n",
    "\n",
    "print('encoder', encoder)\n",
    "print('encoder_last', encoder_last)\n",
    "\n",
    "decoder = Embedding(dict_size, 128, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)\n",
    "decoder = LSTM(512, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n",
    "\n",
    "print('decoder', decoder)\n",
    "\n",
    "# For the plain Sequence-to-Sequence, we produced the output from directly from decoder\n",
    "# output = TimeDistributed(Dense(output_dict_size, activation=\"softmax\"))(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention Tensor(\"attention/truediv:0\", shape=(?, 20, 20), dtype=float32)\n",
      "context Tensor(\"dot_2/MatMul:0\", shape=(?, 20, 512), dtype=float32)\n",
      "decoder_combined_context Tensor(\"concatenate_1/concat:0\", shape=(?, 20, 1024), dtype=float32)\n",
      "output Tensor(\"time_distributed_2/Reshape_1:0\", shape=(?, 20, 5193), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation, dot, concatenate\n",
    "\n",
    "# Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "# Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "attention = dot([decoder, encoder], axes=[2, 2])\n",
    "attention = Activation('softmax', name='attention')(attention)\n",
    "print('attention', attention)\n",
    "\n",
    "context = dot([attention, encoder], axes=[2,1])\n",
    "print('context', context)\n",
    "\n",
    "decoder_combined_context = concatenate([context, decoder])\n",
    "print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "# Has another weight + tanh layer as described in equation (5) of the paper\n",
    "output = TimeDistributed(Dense(512, activation=\"tanh\"))(decoder_combined_context)\n",
    "output = TimeDistributed(Dense(dict_size, activation=\"softmax\"))(output)\n",
    "print('output', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 20, 128)      664704      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 128)      664704      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 20, 512)      1312768     embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 20, 512)      1312768     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 20, 20)       0           lstm_2[0][0]                     \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, 20, 20)       0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 20, 512)      0           attention[0][0]                  \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 20, 1024)     0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 20, 512)      524800      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 20, 5193)     2664009     time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 7,143,753\n",
      "Trainable params: 7,143,753\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_encoder_input = encoded_training_input\n",
    "training_decoder_input = np.zeros_like(encoded_training_output)\n",
    "training_decoder_input[:, 1:] = encoded_training_output[:,:-1]\n",
    "training_decoder_input[:, 0] = WORD_CODE_START\n",
    "training_decoder_output = np.eye(dict_size)[encoded_training_output.astype('int')]\n",
    "\n",
    "validation_encoder_input = encoded_validation_input\n",
    "validation_decoder_input = np.zeros_like(encoded_validation_output)\n",
    "validation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\n",
    "validation_decoder_input[:, 0] = WORD_CODE_START\n",
    "validation_decoder_output = np.eye(dict_size)[encoded_validation_output.astype('int')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 9630 samples, validate on 2406 samples\n",
      "Epoch 1/100\n",
      "9630/9630 [==============================] - 38s 4ms/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 2/100\n",
      "9630/9630 [==============================] - 22s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 3/100\n",
      "9630/9630 [==============================] - 22s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 4/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 5/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 6/100\n",
      "9630/9630 [==============================] - 22s 2ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 7/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 8/100\n",
      "9630/9630 [==============================] - 22s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 9/100\n",
      "9630/9630 [==============================] - 22s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 10/100\n",
      "9630/9630 [==============================] - 22s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 11/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 12/100\n",
      "9630/9630 [==============================] - 22s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 13/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 14/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 15/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 16/100\n",
      "9630/9630 [==============================] - 22s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 17/100\n",
      "9630/9630 [==============================] - 22s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 18/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 19/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 20/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 21/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 22/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 23/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 24/100\n",
      "9630/9630 [==============================] - 22s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 25/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 26/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 27/100\n",
      "9630/9630 [==============================] - 22s 2ms/step - loss: 9.8964e-04 - val_loss: 0.0011\n",
      "Epoch 28/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 9.7752e-04 - val_loss: 0.0011\n",
      "Epoch 29/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 9.6573e-04 - val_loss: 0.0011\n",
      "Epoch 30/100\n",
      "9630/9630 [==============================] - 22s 2ms/step - loss: 9.5339e-04 - val_loss: 0.0011\n",
      "Epoch 31/100\n",
      "9630/9630 [==============================] - 22s 2ms/step - loss: 9.3967e-04 - val_loss: 0.0012\n",
      "Epoch 32/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 9.2702e-04 - val_loss: 0.0012\n",
      "Epoch 33/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 9.1333e-04 - val_loss: 0.0012\n",
      "Epoch 34/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 8.9899e-04 - val_loss: 0.0012\n",
      "Epoch 35/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 8.8411e-04 - val_loss: 0.0012\n",
      "Epoch 36/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 8.6910e-04 - val_loss: 0.0012\n",
      "Epoch 37/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 8.5335e-04 - val_loss: 0.0012\n",
      "Epoch 38/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 8.3757e-04 - val_loss: 0.0012\n",
      "Epoch 39/100\n",
      "9630/9630 [==============================] - 23s 2ms/step - loss: 8.2061e-04 - val_loss: 0.0012\n",
      "Epoch 40/100\n",
      "1728/9630 [====>.........................] - ETA: 15s - loss: 7.9334e-04"
     ]
    }
   ],
   "source": [
    "model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],\n",
    "          validation_data=([validation_encoder_input, validation_decoder_input], [validation_decoder_output]),\n",
    "          #validation_split=0.05,\n",
    "          batch_size=64, epochs=100)\n",
    "\n",
    "model.save('model_attention.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(raw_input):\n",
    "    input_tok = [nltk.word_tokenize(raw_input)]\n",
    "    input_tok = [input_tok[0][::-1]]  #reverseing input seq\n",
    "    encoder_input = transform(encoding, input_tok, 20)\n",
    "    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n",
    "    decoder_input[:,0] = WORD_CODE_START\n",
    "    for i in range(1, OUTPUT_LENGTH):\n",
    "        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n",
    "        decoder_input[:,i] = output[:,i]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode(decoding, vector):\n",
    "    text = ''\n",
    "    for i in vector:\n",
    "        if i == 0:\n",
    "            break\n",
    "        text += ' '\n",
    "        text += decoding[i]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: how many men aremain in castle black\n",
      "A:  they do not belong down here\n",
      "Q: tyrions chamber\n",
      "A:  you will be pleased to know our mutual friend is doing quite well in lady sansas service\n",
      "Q: melisandre stannis davos\n",
      "A:  the prince doe not wish to be disturbed\n",
      "Q: you are practicing for a fight you should practice right\n",
      "A:  you are a champion of the common people lord snow what do you say to brother <UNK> proposition\n",
      "Q: it would take you awhile with a sparring sword\n",
      "A:  i will find them\n",
      "Q: what are you doing here\n",
      "A:  i am not a beggar i live here\n",
      "Q: my lord\n",
      "A:  you are not wasting away are a lovely boy for the lad i am right you else\n",
      "Q: why are you cry\n",
      "A:  i do not know\n",
      "Q: if that is true then slit my throat and be done with it\n",
      "A:  i am not your little princess i am daenerys stormborn of the blood of old valyria and i will will\n",
      "Q: people would not like that\n",
      "A:  you are a liar and if you told the truth mycah would be alive\n",
      "Q: still no word\n",
      "A:  a you know your grace\n",
      "Q: only varys i am not actually a nobleman no one is under obligation to call me lord\n",
      "A:  i am a hostage and you are selling me\n",
      "Q: i believe you grow more beautiful every day lady sansa\n",
      "A:  i am a man of the night\n",
      "Q: what do you want lord baelish\n",
      "A:  i have been a bit busy\n",
      "Q: it is a long way down\n",
      "A:  i wa trying to learn i asked mycah to practice with me i asked him it wa my fault fault\n",
      "Q: and i suppose you want me to broker this agreement\n",
      "A:  you are a champion of the common people lord snow what do you say it is not you you you\n",
      "Q: he is one of them now the wall ha fallen the dead march south\n",
      "A:  i am not a cripple\n",
      "Q: widow wail i like that every time i use it itll be like cutting off ned starks head all over again\n",
      "A:  i am a hostage and you are selling me\n",
      "Q: and what is my role in all this\n",
      "A:  i will find her for lady catelyn and for you\n",
      "Q: on the morning of my 18th nameday my father came to me you are almost a man now he said but you are not worthy of my land and title tomorrow you are going to take the black forsake all claim to your inheritance and start north if you do not he said then well have a hunt and somewhere in these wood your horse will stumble and you will be thrown from your saddle to die or so i will tell your mother nothing would please me more ser allisers going to make me fight again tomorrow is not he\n",
      "A:  i am a hostage and you are selling me\n"
     ]
    }
   ],
   "source": [
    "question=[]\n",
    "answer=[]\n",
    "for i in range(20):\n",
    "    seq_index = np.random.randint(1, len(short_questions))\n",
    "    output = prediction(short_questions[seq_index])\n",
    "    print ('Q:', short_questions[seq_index])\n",
    "    print ('A:', decode(decoding, output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=[]\n",
    "answer=[]\n",
    "for i in range(20):\n",
    "    seq_index = np.random.randint(1, len(short_questions))\n",
    "    output = prediction(short_questions[seq_index])\n",
    "    question.append(short_questions[seq_index])\n",
    "    answer.append(decode(decoding, output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "zippedList =  list(zip(question, answer))\n",
    "\n",
    "data = pd.DataFrame(zippedList, columns = ['Questions','Answers'])\n",
    "data.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For input in kernel only\n",
    "#raw_input = input()\n",
    "#output = prediction(raw_input)\n",
    "#print (decode(decoding, output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
