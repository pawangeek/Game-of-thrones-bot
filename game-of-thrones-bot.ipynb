{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ansfile.txt', 'quesfile.txt', 'finalgot.txt']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import SimpleRNN\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "INPUT_LENGTH = 20\n",
    "OUTPUT_LENGTH = 20\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "fn1 = open('../input/quesfile.txt', 'r')\n",
    "short_questions = fn1.readlines()\n",
    "short_questions = [x.lower() for x in short_questions]\n",
    "fn1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn2 = open('../input/ansfile.txt', 'r')\n",
    "short_answers = fn2.readlines()\n",
    "short_answers = [x.lower() for x in short_answers]\n",
    "fn2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing number of samples\n",
    "num_samples = 30000  # Number of samples to train on.\n",
    "short_questions = short_questions[:num_samples]\n",
    "short_answers = short_answers[:num_samples]\n",
    "#tokenizing the qns and answers\n",
    "short_questions_tok = [nltk.word_tokenize(sent) for sent in short_questions]\n",
    "short_answers_tok = [nltk.word_tokenize(sent) for sent in short_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size 9630\n",
      "validation size 2407\n"
     ]
    }
   ],
   "source": [
    "#train-validation split\n",
    "data_size = len(short_questions_tok)\n",
    "\n",
    "# We will use the first 0-80th %-tile (80%) of data for the training\n",
    "training_input  = short_questions_tok[:round(data_size*(80/100))]\n",
    "training_input  = [tr_input[::-1] for tr_input in training_input] #reverseing input seq for better performance\n",
    "training_output = short_answers_tok[:round(data_size*(80/100))]\n",
    "\n",
    "# We will use the remaining for validation\n",
    "validation_input = short_questions_tok[round(data_size*(80/100)):]\n",
    "validation_input  = [val_input[::-1] for val_input in validation_input] #reverseing input seq for better performance\n",
    "validation_output = short_answers_tok[round(data_size*(80/100)):]\n",
    "\n",
    "print('training size', len(training_input))\n",
    "print('validation size', len(validation_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for the frequency of the vocabulary\n",
    "# Create \n",
    "vocab = {}\n",
    "for question in short_questions_tok:\n",
    "    for word in question:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "\n",
    "for answer in short_answers_tok:\n",
    "    for word in answer:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare words from the vocabulary.\n",
    "# We will aim to replace fewer than 5% of words with <UNK>\n",
    "# You will see this ratio soon.\n",
    "threshold = 2\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of total vocab: 9567\n",
      "Size of vocab we will use: 5841\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of total vocab:\", len(vocab))\n",
    "print(\"Size of vocab we will use:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of vocab used: 5843\n"
     ]
    }
   ],
   "source": [
    "#we will create dictionaries to provide a unique integer for each word.\n",
    "WORD_CODE_START = 1\n",
    "WORD_CODE_PADDING = 0\n",
    "\n",
    "word_num  = 2 #number 1 is left for WORD_CODE_START for model decoder later\n",
    "encoding = {}\n",
    "decoding = {1: 'START'}\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold: #get vocabularies that appear above threshold count\n",
    "        encoding[word] = word_num \n",
    "        decoding[word_num ] = word\n",
    "        word_num += 1\n",
    "\n",
    "print(\"No. of vocab used:\", word_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5844"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#include unknown token for words not in dictionary\n",
    "decoding[len(encoding)+2] = '<UNK>'\n",
    "encoding['<UNK>'] = len(encoding)+2\n",
    "\n",
    "dict_size = word_num+1\n",
    "dict_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(encoding, data, vector_size=20):\n",
    "    transformed_data = np.zeros(shape=(len(data), vector_size))\n",
    "    for i in range(len(data)):\n",
    "        for j in range(min(len(data[i]), vector_size)):\n",
    "            try:\n",
    "                transformed_data[i][j] = encoding[data[i][j]]\n",
    "            except:\n",
    "                transformed_data[i][j] = encoding['<UNK>']\n",
    "    return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_training_input (9630, 20)\n",
      "encoded_training_output (9630, 20)\n"
     ]
    }
   ],
   "source": [
    "#encoding training set\n",
    "encoded_training_input = transform(\n",
    "    encoding, training_input, vector_size=INPUT_LENGTH)\n",
    "encoded_training_output = transform(\n",
    "    encoding, training_output, vector_size=OUTPUT_LENGTH)\n",
    "\n",
    "print('encoded_training_input', encoded_training_input.shape)\n",
    "print('encoded_training_output', encoded_training_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_validation_input (2406, 20)\n",
      "encoded_validation_output (2406, 20)\n"
     ]
    }
   ],
   "source": [
    "validation_input.pop()\n",
    "#encoding validation set\n",
    "encoded_validation_input = transform(\n",
    "    encoding, validation_input, vector_size=INPUT_LENGTH)\n",
    "encoded_validation_output = transform(\n",
    "    encoding, validation_output, vector_size=OUTPUT_LENGTH)\n",
    "\n",
    "print('encoded_validation_input', encoded_validation_input.shape)\n",
    "print('encoded_validation_output', encoded_validation_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "encoder_input = Input(shape=(INPUT_LENGTH,))\n",
    "decoder_input = Input(shape=(OUTPUT_LENGTH,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "encoder Tensor(\"lstm_1/transpose_2:0\", shape=(?, 20, 512), dtype=float32)\n",
      "encoder_last Tensor(\"strided_slice:0\", shape=(?, 512), dtype=float32)\n",
      "decoder Tensor(\"lstm_2/transpose_2:0\", shape=(?, 20, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "encoder = Embedding(dict_size, 128, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)\n",
    "encoder = LSTM(512, return_sequences=True, unroll=True)(encoder)\n",
    "encoder_last = encoder[:,-1,:]\n",
    "\n",
    "print('encoder', encoder)\n",
    "print('encoder_last', encoder_last)\n",
    "\n",
    "decoder = Embedding(dict_size, 128, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)\n",
    "decoder = LSTM(512, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n",
    "\n",
    "print('decoder', decoder)\n",
    "\n",
    "# For the plain Sequence-to-Sequence, we produced the output from directly from decoder\n",
    "# output = TimeDistributed(Dense(output_dict_size, activation=\"softmax\"))(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention Tensor(\"attention/truediv:0\", shape=(?, 20, 20), dtype=float32)\n",
      "context Tensor(\"dot_2/MatMul:0\", shape=(?, 20, 512), dtype=float32)\n",
      "decoder_combined_context Tensor(\"concatenate_1/concat:0\", shape=(?, 20, 1024), dtype=float32)\n",
      "output Tensor(\"time_distributed_2/Reshape_1:0\", shape=(?, 20, 5844), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Activation, dot, concatenate\n",
    "\n",
    "# Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
    "# Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "attention = dot([decoder, encoder], axes=[2, 2])\n",
    "attention = Activation('softmax', name='attention')(attention)\n",
    "print('attention', attention)\n",
    "\n",
    "context = dot([attention, encoder], axes=[2,1])\n",
    "print('context', context)\n",
    "\n",
    "decoder_combined_context = concatenate([context, decoder])\n",
    "print('decoder_combined_context', decoder_combined_context)\n",
    "\n",
    "# Has another weight + tanh layer as described in equation (5) of the paper\n",
    "output = TimeDistributed(Dense(512, activation=\"tanh\"))(decoder_combined_context)\n",
    "output = TimeDistributed(Dense(dict_size, activation=\"softmax\"))(output)\n",
    "print('output', output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 20, 128)      748032      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 128)      748032      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 20, 512)      1312768     embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 20, 512)      1312768     embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 20, 20)       0           lstm_2[0][0]                     \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, 20, 20)       0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dot_2 (Dot)                     (None, 20, 512)      0           attention[0][0]                  \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 20, 1024)     0           dot_2[0][0]                      \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 20, 512)      524800      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 20, 5844)     2997972     time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 7,644,372\n",
      "Trainable params: 7,644,372\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_encoder_input = encoded_training_input\n",
    "training_decoder_input = np.zeros_like(encoded_training_output)\n",
    "training_decoder_input[:, 1:] = encoded_training_output[:,:-1]\n",
    "training_decoder_input[:, 0] = WORD_CODE_START\n",
    "training_decoder_output = np.eye(dict_size)[encoded_training_output.astype('int')]\n",
    "\n",
    "validation_encoder_input = encoded_validation_input\n",
    "validation_decoder_input = np.zeros_like(encoded_validation_output)\n",
    "validation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\n",
    "validation_decoder_input[:, 0] = WORD_CODE_START\n",
    "validation_decoder_output = np.eye(dict_size)[encoded_validation_output.astype('int')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 9630 samples, validate on 2406 samples\n",
      "Epoch 1/100\n",
      "9630/9630 [==============================] - 36s 4ms/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 2/100\n",
      "9630/9630 [==============================] - 21s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 3/100\n",
      "9630/9630 [==============================] - 21s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 4/100\n",
      "9630/9630 [==============================] - 21s 2ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 5/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 6/100\n",
      "9630/9630 [==============================] - 21s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 7/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 8/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 9/100\n",
      "9630/9630 [==============================] - 21s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 10/100\n",
      "9630/9630 [==============================] - 21s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 11/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 12/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 13/100\n",
      "9630/9630 [==============================] - 21s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 14/100\n",
      "9630/9630 [==============================] - 21s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 15/100\n",
      "9630/9630 [==============================] - 21s 2ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 16/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 9.9207e-04 - val_loss: 0.0010\n",
      "Epoch 17/100\n",
      "9630/9630 [==============================] - 21s 2ms/step - loss: 9.8271e-04 - val_loss: 0.0010\n",
      "Epoch 18/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 9.7360e-04 - val_loss: 0.0010\n",
      "Epoch 19/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 9.6532e-04 - val_loss: 0.0010\n",
      "Epoch 20/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 9.5610e-04 - val_loss: 0.0010\n",
      "Epoch 21/100\n",
      "9630/9630 [==============================] - 21s 2ms/step - loss: 9.4772e-04 - val_loss: 0.0010\n",
      "Epoch 22/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 9.3906e-04 - val_loss: 0.0010\n",
      "Epoch 23/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 9.3066e-04 - val_loss: 0.0010\n",
      "Epoch 24/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 9.2160e-04 - val_loss: 0.0010\n",
      "Epoch 25/100\n",
      "9630/9630 [==============================] - 21s 2ms/step - loss: 9.1270e-04 - val_loss: 0.0010\n",
      "Epoch 26/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 9.0357e-04 - val_loss: 0.0010\n",
      "Epoch 27/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 8.9394e-04 - val_loss: 0.0010\n",
      "Epoch 28/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 8.8450e-04 - val_loss: 0.0010\n",
      "Epoch 29/100\n",
      "9630/9630 [==============================] - 21s 2ms/step - loss: 8.7448e-04 - val_loss: 0.0010\n",
      "Epoch 30/100\n",
      "9630/9630 [==============================] - 21s 2ms/step - loss: 8.6463e-04 - val_loss: 0.0010\n",
      "Epoch 31/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 8.5401e-04 - val_loss: 0.0010\n",
      "Epoch 32/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 8.4335e-04 - val_loss: 0.0010\n",
      "Epoch 33/100\n",
      "9630/9630 [==============================] - 21s 2ms/step - loss: 8.3279e-04 - val_loss: 0.0010\n",
      "Epoch 34/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 8.2120e-04 - val_loss: 0.0010\n",
      "Epoch 35/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 8.0952e-04 - val_loss: 0.0010\n",
      "Epoch 36/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 7.9734e-04 - val_loss: 0.0010\n",
      "Epoch 37/100\n",
      "9630/9630 [==============================] - 21s 2ms/step - loss: 7.8586e-04 - val_loss: 0.0010\n",
      "Epoch 38/100\n",
      "9630/9630 [==============================] - 20s 2ms/step - loss: 7.7286e-04 - val_loss: 0.0011\n",
      "Epoch 39/100\n",
      "9630/9630 [==============================] - 21s 2ms/step - loss: 7.5981e-04 - val_loss: 0.0011\n",
      "Epoch 40/100\n",
      "2304/9630 [======>.......................] - ETA: 13s - loss: 7.4065e-04"
     ]
    }
   ],
   "source": [
    "model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],\n",
    "          validation_data=([validation_encoder_input, validation_decoder_input], [validation_decoder_output]),\n",
    "          #validation_split=0.05,\n",
    "          batch_size=64, epochs=100)\n",
    "\n",
    "model.save('model_attention.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(raw_input):\n",
    "    input_tok = [nltk.word_tokenize(raw_input)]\n",
    "    input_tok = [input_tok[0][::-1]]  #reverseing input seq\n",
    "    encoder_input = transform(encoding, input_tok, 20)\n",
    "    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n",
    "    decoder_input[:,0] = WORD_CODE_START\n",
    "    for i in range(1, OUTPUT_LENGTH):\n",
    "        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n",
    "        decoder_input[:,i] = output[:,i]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode(decoding, vector):\n",
    "    text = ''\n",
    "    for i in vector:\n",
    "        if i == 0:\n",
    "            break\n",
    "        text += ' '\n",
    "        text += decoding[i]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  i do not know what that is\n",
      "\n",
      "A:  what is dead may never die\n",
      "Q:  without you there to rule khaleesi i fear the masters will simply bide their time wait for the invaders to leave and areassert control\n",
      "\n",
      "A:  you are a good man\n",
      "Q:  arenlys death was no fault of yours you served him bravely\n",
      "\n",
      "A:  i am not afraid of them i am not going to run\n",
      "Q:  it was never our forte golden roses indeed your brother and his new queen thought you would be defending casterly rock\n",
      "\n",
      "A:  you are a pretty lad the girls would claw each others eyes out to get naked with you\n",
      "Q:  good point\n",
      "\n",
      "A:  they say he rides into battle on the back of a giant direwolf they say he can turn into into\n",
      "Q:  now and always\n",
      "\n",
      "A:  the prince myrcella\n",
      "Q:  i have nowhere else to go i have taken her to every healer in braavos i have spent every penny i had she suffers every day of her life i just want it to end\n",
      "\n",
      "A:  you are a pretty man\n",
      "Q:  a field in the north\n",
      "\n",
      "A:  the north the abandoned nightfort night\n",
      "Q:   the lord has smiled upon me\n",
      "\n",
      "A:  you are not a knight\n",
      "Q:  this is exactly what they want\n",
      "\n",
      "A:  you are not a knight\n",
      "Q:  who are you\n",
      "\n",
      "A:  i am not scared\n",
      "Q:  but perhaps the gambler loses his bet and decides that he does not have to pay after all a destitute woman and her small child what can they do to such a man if he keeps their money for himself to whom can they turn for arecourse\n",
      "\n",
      "A:  you are a pretty lad the girls would claw each others eyes out to get naked with you\n",
      "Q:  i am grateful for your loyalty but my dragon died so that we could be here if it is all for nothing then he died for nothing\n",
      "\n",
      "A:  you are a pretty lad the girls would claw each others eyes out to get naked with you\n",
      "Q:  but you were accused\n",
      "\n",
      "A:  i am not sure you understand how much people hate your family in this part of the world\n",
      "Q:  that is no place for a king\n",
      "\n",
      "A:  you are not a boy\n",
      "Q:  maybe not but i could carve her heart out\n",
      "\n",
      "A:  you are not a boy anymore they attacked your father they have already started the war it is your your\n",
      "Q:  i am child i disobeyed my king your father and now i am paying the price\n",
      "\n",
      "A:  you are a good man lord tyrell but i do not i amagine you will have the time in in\n",
      "Q:  what sort of doom does the king face\n",
      "\n",
      "A:  he is not aready\n",
      "Q:  he should be with his grandfather his brother his sister burn him and bury his ashes where the sept once stood\n",
      "\n",
      "A:  you are a pretty lad the girls would claw each others eyes out to get naked with you\n",
      "Q:  fine and heres my youngest daughter shirei though she hasnt bled yet clearly you do not have the patience for all that\n",
      "\n",
      "A:  you are a king and that means you do not have to do everything yourself\n"
     ]
    }
   ],
   "source": [
    "question=[]\n",
    "answer=[]\n",
    "for i in range(20):\n",
    "    seq_index = np.random.randint(1, len(short_questions))\n",
    "    output = prediction(short_questions[seq_index])\n",
    "    print ('Q:', short_questions[seq_index])\n",
    "    print ('A:', decode(decoding, output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=[]\n",
    "answer=[]\n",
    "for i in range(20):\n",
    "    seq_index = np.random.randint(1, len(short_questions))\n",
    "    output = prediction(short_questions[seq_index])\n",
    "    question.append(short_questions[seq_index])\n",
    "    answer.append(decode(decoding, output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "zippedList =  list(zip(question, answer))\n",
    "\n",
    "data = pd.DataFrame(zippedList, columns = ['Questions','Answers'])\n",
    "data.to_csv('data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For input in kernel only\n",
    "#raw_input = input()\n",
    "#output = prediction(raw_input)\n",
    "#print (decode(decoding, output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
